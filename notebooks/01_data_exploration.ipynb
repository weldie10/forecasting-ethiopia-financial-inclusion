{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 1: Data Exploration and Enrichment\n",
        "\n",
        "## Objective\n",
        "Understand the starter dataset and enrich it with additional data useful for forecasting financial inclusion in Ethiopia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Explore Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import logging\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append(str(Path('../src').resolve()))\n",
        "\n",
        "# Import Task1 classes\n",
        "from task1_data_exploration import (\n",
        "    Task1DataProcessor,\n",
        "    DataExplorer,\n",
        "    DataEnricher,\n",
        "    ObservationRecord,\n",
        "    EventRecord,\n",
        "    ImpactLinkRecord,\n",
        "    ConfidenceLevel,\n",
        "    RecordType,\n",
        "    ImpactDirection\n",
        ")\n",
        "\n",
        "# Set up paths\n",
        "data_dir = Path('../data/raw')\n",
        "processed_dir = Path('../data/processed')\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "\n",
        "# Initialize the processor\n",
        "processor = Task1DataProcessor(\n",
        "    data_file=data_dir / 'ethiopia_fi_unified_data.xlsx',\n",
        "    reference_codes_file=data_dir / 'reference_codes.xlsx',\n",
        "    logger=logger\n",
        ")\n",
        "\n",
        "logger.info(\"Notebook initialized successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Load Main Dataset (ethiopia_fi_unified_data.xlsx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load all datasets using the processor\n",
        "data_df, impact_links_df, reference_codes_df = processor.explorer.load_data()\n",
        "\n",
        "print(f\"Data sheet shape: {data_df.shape}\")\n",
        "print(f\"Impact links shape: {impact_links_df.shape}\")\n",
        "print(f\"Reference codes shape: {reference_codes_df.shape}\")\n",
        "print(f\"\\nData columns: {list(data_df.columns)}\")\n",
        "print(f\"\\nFirst few rows of data:\")\n",
        "data_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display impact links\n",
        "print(f\"Impact links shape: {impact_links_df.shape}\")\n",
        "print(f\"\\nColumns: {list(impact_links_df.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "impact_links_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Load Reference Codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display reference codes\n",
        "print(f\"Reference codes shape: {reference_codes_df.shape}\")\n",
        "print(f\"\\nColumns: {list(reference_codes_df.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "reference_codes_df.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Understand the Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get schema information using the processor\n",
        "schema_info = processor.explorer.get_schema_info()\n",
        "\n",
        "print(\"=== DATA SHEET INFO ===\")\n",
        "print(f\"Shape: {schema_info['data_shape']}\")\n",
        "print(f\"Columns: {schema_info['data_columns']}\")\n",
        "print(f\"\\nData types:\")\n",
        "for col, dtype in schema_info['data_dtypes'].items():\n",
        "    print(f\"  {col}: {dtype}\")\n",
        "\n",
        "print(\"\\n=== IMPACT LINKS INFO ===\")\n",
        "print(f\"Shape: {schema_info['impact_links_shape']}\")\n",
        "print(f\"Columns: {schema_info['impact_links_columns']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get missing values using the processor\n",
        "missing = processor.explorer.get_missing_values()\n",
        "\n",
        "print(\"=== MISSING VALUES IN DATA SHEET ===\")\n",
        "missing_data = missing['data']\n",
        "print(missing_data[missing_data > 0])\n",
        "\n",
        "print(\"\\n=== MISSING VALUES IN IMPACT LINKS ===\")\n",
        "missing_links = missing['impact_links']\n",
        "print(missing_links[missing_links > 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Explore the Data\n",
        "\n",
        "### 3.1 Count Records by Type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count by record_type using the processor\n",
        "print(\"=== RECORD TYPE DISTRIBUTION ===\")\n",
        "record_type_counts = processor.explorer.count_by_record_type()\n",
        "print(record_type_counts)\n",
        "print(f\"\\nTotal records: {len(processor.explorer.data_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count by pillar using the processor\n",
        "print(\"=== PILLAR DISTRIBUTION ===\")\n",
        "pillar_counts = processor.explorer.count_by_pillar()\n",
        "print(pillar_counts)\n",
        "if len(processor.explorer.data_df) > 0:\n",
        "    null_count = processor.explorer.data_df['pillar'].isnull().sum()\n",
        "    print(f\"\\nRecords with null pillar: {null_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count by source_type and confidence using the processor\n",
        "print(\"=== SOURCE TYPE DISTRIBUTION ===\")\n",
        "source_type_counts = processor.explorer.count_by_source_type()\n",
        "print(source_type_counts)\n",
        "\n",
        "print(\"\\n=== CONFIDENCE DISTRIBUTION ===\")\n",
        "confidence_counts = processor.explorer.count_by_confidence()\n",
        "print(confidence_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Temporal Range Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get temporal range using the processor\n",
        "temporal_info = processor.explorer.get_temporal_range()\n",
        "\n",
        "for col, info in temporal_info.items():\n",
        "    print(f\"\\n=== {col.upper()} ===\")\n",
        "    print(f\"Min: {info['min']}\")\n",
        "    print(f\"Max: {info['max']}\")\n",
        "    print(f\"Non-null count: {info['non_null_count']}\")\n",
        "    print(f\"Null count: {info['null_count']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Unique Indicators Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get unique indicators using the processor\n",
        "print(\"=== UNIQUE INDICATOR CODES ===\")\n",
        "unique_indicators = processor.explorer.get_unique_indicators()\n",
        "print(f\"Total unique indicators: {len(unique_indicators)}\")\n",
        "print(f\"\\nIndicator coverage (top 20):\")\n",
        "print(unique_indicators.head(20))\n",
        "\n",
        "# Show all unique indicator codes\n",
        "if len(processor.explorer.data_df) > 0 and 'indicator_code' in processor.explorer.data_df.columns:\n",
        "    print(f\"\\nAll unique indicator codes:\")\n",
        "    print(sorted(processor.explorer.data_df['indicator_code'].dropna().unique()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get indicator coverage by pillar using the processor\n",
        "print(\"=== INDICATOR COVERAGE BY PILLAR ===\")\n",
        "coverage = processor.explorer.get_indicator_coverage_by_pillar()\n",
        "print(coverage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Events Catalog Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get events catalog using the processor\n",
        "events = processor.explorer.get_events_catalog()\n",
        "print(f\"=== EVENTS CATALOG ===\")\n",
        "print(f\"Total events: {len(events)}\")\n",
        "\n",
        "if len(events) > 0:\n",
        "    events_by_category = processor.explorer.get_events_by_category()\n",
        "    print(f\"\\nEvent categories:\")\n",
        "    print(events_by_category)\n",
        "    \n",
        "    # Show events with dates\n",
        "    date_col = [col for col in events.columns if 'date' in col.lower()][0] if [col for col in events.columns if 'date' in col.lower()] else None\n",
        "    if date_col:\n",
        "        print(f\"\\nEvents by date (first 20):\")\n",
        "        events_sorted = events.sort_values(date_col)\n",
        "        display_cols = [col for col in ['category', date_col, 'pillar'] if col in events.columns]\n",
        "        print(events_sorted[display_cols].head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.5 Impact Links Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get impact links summary using the processor\n",
        "print(\"=== IMPACT LINKS OVERVIEW ===\")\n",
        "summary = processor.explorer.get_impact_links_summary()\n",
        "print(f\"Total impact links: {summary['total_links']}\")\n",
        "\n",
        "if 'by_pillar' in summary:\n",
        "    print(f\"\\nImpact links by pillar:\")\n",
        "    print(summary['by_pillar'])\n",
        "\n",
        "if 'by_direction' in summary:\n",
        "    print(f\"\\nImpact direction distribution:\")\n",
        "    print(summary['by_direction'])\n",
        "\n",
        "if 'unique_parent_events' in summary:\n",
        "    print(f\"\\nUnique parent events linked: {summary['unique_parent_events']}\")\n",
        "    if 'top_linked_events' in summary:\n",
        "        print(f\"\\nTop events by number of links:\")\n",
        "        print(summary['top_linked_events'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Quality Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate data quality using the processor\n",
        "print(\"=== DATA QUALITY CHECKS ===\")\n",
        "quality_issues = processor.explorer.validate_data_quality()\n",
        "\n",
        "print(f\"\\nDuplicate records in data sheet: {quality_issues['duplicates_data']}\")\n",
        "print(f\"Duplicate records in impact links: {quality_issues['duplicates_impact_links']}\")\n",
        "print(f\"Invalid pillar values: {quality_issues['invalid_pillars']}\")\n",
        "\n",
        "if 'invalid_pillar_values' in quality_issues:\n",
        "    print(f\"\\nInvalid pillar value counts:\")\n",
        "    print(quality_issues['invalid_pillar_values'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get comprehensive summary statistics using the processor\n",
        "print(\"=== DATASET SUMMARY ===\")\n",
        "summary = processor.explorer.get_summary_statistics()\n",
        "for key, value in summary.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "# Run full exploration to get all results at once\n",
        "print(\"\\n=== RUNNING FULL EXPLORATION ===\")\n",
        "full_results = processor.run_full_exploration()\n",
        "print(\"Full exploration complete! All results stored in 'full_results' variable.\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
